{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# note-1-train-and-test\n",
    "This notebook is for main task of training and testing the model. \n",
    "It loads the pre-trained model and train it on the MNIST dataset.\n",
    "\n",
    "## Flow\n",
    "1. Setup the environment\n",
    "1. Train the model\n",
    "1. Test the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step\n",
    "\n",
    "### 1. Setup the environment\n",
    "#### Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries and set the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "\n",
    "# Config\n",
    "FORCE_CPU = True\n",
    "FREEZE_RESNET = False\n",
    "CHANNEL_SIZE = 3\n",
    "RESIZE_SIZE = (224, 224)\n",
    "\n",
    "NORMALIZE_MEAN = (0.5,0.5,0.5)\n",
    "NORMALIZE_STD = (0.5,0.5,0.5)\n",
    "\n",
    "DATA_ROOT_PATH = \"./data\"\n",
    "MODEL_ROOT_PATH = \"./models\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "num_workers = 0 # main process\n",
    "t_max = 200\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Grayscale(num_output_channels=CHANNEL_SIZE),\n",
    "        torchvision.transforms.Resize(RESIZE_SIZE),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(NORMALIZE_MEAN,NORMALIZE_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if FORCE_CPU:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU is forced. Using CPU.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available and built. Using Apple Silicon GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Neither CUDA nor MPS is available or built. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torchvision.datasets.MNIST(root=DATA_ROOT_PATH, train=True, download=True, transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "dataset_test = torchvision.datasets.MNIST(root=DATA_ROOT_PATH, train=False, download=True, transform=transform)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info.\n",
    "print(f\"Training Dataset: {len(dataset_train)} samples\")\n",
    "print(f\"Testing Dataset: {len(dataset_test)} samples\")\n",
    "\n",
    "# Visualize a sample\n",
    "sample_train = next(iter(dataloader_train))\n",
    "sample_test = next(iter(dataloader_test))\n",
    "\n",
    "img_train = sample_train[0][0].permute(1, 2, 0) # (C, H, W) -> (H, W, C)\n",
    "img_train = (img_train + 1) / 2 # Normalize Grayscale to [0, 1]\n",
    "\n",
    "img_test = sample_test[0][0].permute(1, 2, 0) # (C, H, W) -> (H, W, C)\n",
    "img_test = (img_test + 1) / 2 # Normalize Grayscale to [0, 1]\n",
    "\n",
    "plt.imshow(img_train)\n",
    "plt.title(f\"Training sample (Label: {sample_train[1][0]})\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img_test)\n",
    "plt.title(f\"Testing sample (Label: {sample_test[1][0]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model\n",
    "\n",
    "#### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model : torch.nn.Module,\n",
    "    dataloader : torch.utils.data.DataLoader,\n",
    "    criterion : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    show_progress : bool = True,\n",
    "    epoch : int = 0,\n",
    "    num_epochs : int = 0,\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if show_progress:\n",
    "            print(f\"  -- batch [Partial Loss: {loss.item():.4f}, Partial Accuracy: {100*correct/total:.1f}%]\")\n",
    "\n",
    "    total_loss = train_loss / total\n",
    "    total_acc = correct / total\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {100*total_acc:.1f}%\")\n",
    "\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and set the model\n",
    "Load the pre-trained model and set the parameters to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(\n",
    "    weights = torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1\n",
    ")\n",
    "\n",
    "if FREEZE_RESNET:\n",
    "    for name, param in model.named_parameters():\n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Fine-tune the fully connected layer\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset_train.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# # Get summary\n",
    "# from torchsummary import summary\n",
    "# print(\"Pre-trained ResNet18 Summary:\")\n",
    "# summary(model, (3, 224, 224))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    params = model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    momentum = momentum,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_model(model, dataloader_train, criterion, optimizer, show_progress=True, epoch=epoch, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {                       
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_ROOT_PATH):\n",
    "    os.makedirs(MODEL_ROOT_PATH)\n",
    "\n",
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save(f\"{MODEL_ROOT_PATH}/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.jit.load(f\"{MODEL_ROOT_PATH}/model.pt\")\n",
    "loaded_model.eval()\n",
    "loaded_model = loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample\n",
    "sample_test = next(iter(dataloader_test))\n",
    "\n",
    "img_test = sample_test[0][0].permute(1, 2, 0) # (C, H, W) -> (H, W, C)\n",
    "img_test = (img_test + 1) / 2 # Normalize Grayscale to [0, 1]\n",
    "\n",
    "test_img = img_test.permute(2, 0, 1).unsqueeze(dim=0).to(device)  # (H,W,C) -> (1,C,H,W)\n",
    "response = loaded_model(test_img)\n",
    "probabilities = torch.nn.functional.softmax(response, dim=1)\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "plt.imshow(img_test)\n",
    "plt.title(f\"Testing sample (Real: {sample_test[1][0]}, Predicted: {predicted_class})\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predicted Number: {predicted_class}\")\n",
    "print(f\"Probability Distribution:\")\n",
    "for idx, prob in enumerate(probabilities[0]):\n",
    "    print(f\" - Number {idx}: {prob.item()*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pilot-mlops-cicd-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
